(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{154:function(t,s,a){"use strict";a.r(s);var n=a(0),o=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"bfs-与-dfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfs-与-dfs","aria-hidden":"true"}},[t._v("#")]),t._v(" BFS 与 DFS")]),t._v(" "),a("div",{staticClass:"language-java extra-class"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[t._v("    B "),a("span",{attrs:{class:"token operator"}},[t._v("--")]),a("span",{attrs:{class:"token operator"}},[t._v("--")]),t._v(" D\n  "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v("   "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v("  "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v(" \\\nA   "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v("  "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v("   "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v("  F\n  \\ "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v("    "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v("\n    C "),a("span",{attrs:{class:"token operator"}},[t._v("--")]),a("span",{attrs:{class:"token operator"}},[t._v("--")]),t._v(" E\n")])])]),a("h2",{attrs:{id:"bfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfs","aria-hidden":"true"}},[t._v("#")]),t._v(" BFS")]),t._v(" "),a("p",[t._v("比如上面这个无向图，由于可以选择的遍历起始点不同，所以它的 BFS 遍历可能会有 N 种可能性。其中一种可能情况是："),a("code",[t._v("A B C D E F")]),t._v("。")]),t._v(" "),a("p",[t._v("注意，"),a("code",[t._v("A B C E D F")]),t._v(" 并不是它的 BFS 遍历，因为如果 "),a("code",[t._v("B")]),t._v(" 节点比 "),a("code",[t._v("C")]),t._v(" 节点先被遍历到的话，那么在下一层 "),a("code",[t._v("B")]),t._v(" 节点的邻接点将被先遍历到，所以下一层只能是 "),a("code",[t._v("D E")]),t._v(" 的顺序。")]),t._v(" "),a("h3",{attrs:{id:"queue-hashset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#queue-hashset","aria-hidden":"true"}},[t._v("#")]),t._v(" Queue + HashSet")]),t._v(" "),a("p",[t._v("使用 Queue + HashSet 来处理 BFS，是因为 Queue 能保证遍历的层的顺序，而 HashSet 能记录被遍历过的点，避免重复遍历。")]),t._v(" "),a("p",[t._v("假如以 A 点作为起始点，那么这个图的 BFS 遍历过程将是：")]),t._v(" "),a("ol",[a("li",[t._v("A 入列，并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("A")]),t._v(" 出列，遍历 A 的邻接点(B, C)，寻找不在 HashSet 中的点(说明该点还没有被遍历到)，B, C 入列并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("B")]),t._v(" 出列，遍历 B 的邻接点(A, C, D)，寻找不在 HashSet 中的点，D 入列并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("C")]),t._v(" 出列，遍历 C 的邻接点(A, B, E, D)，寻找不在 HashSet 中的点，E 入列并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("D")]),t._v(" 出列，遍历 D 的邻接点(B, C, E, F)，寻找不在 HashSet 中的点，F 入列并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("E")]),t._v(" 出列，遍历 E 的邻接点(C, D, F)，寻找不在 HashSet 中的点，没有符合要求的点。")]),t._v(" "),a("li",[a("code",[t._v("F")]),t._v(" 出列，遍历 F 的邻接点(D, E)，寻找不在 HashSet 中的点，没有符合要求的点。")]),t._v(" "),a("li",[t._v("队列为空，完成遍历。")])]),t._v(" "),a("p",[t._v("所以顺序为："),a("code",[t._v("A -> B -> C -> D -> E -> F")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"bfs-代码"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfs-代码","aria-hidden":"true"}},[t._v("#")]),t._v(" BFS 代码")]),t._v(" "),a("div",{staticClass:"language-java extra-class"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("BFS")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GraphNode graph"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GraphNode root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  Queue"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" queue "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("LinkedList")]),a("span",{attrs:{class:"token operator"}},[t._v("<")]),a("span",{attrs:{class:"token operator"}},[t._v(">")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  Set"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" set "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("HashSet")]),a("span",{attrs:{class:"token operator"}},[t._v("<")]),a("span",{attrs:{class:"token operator"}},[t._v(">")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  queue"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("offer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("add")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token operator"}},[t._v("!")]),t._v("queue"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("isEmpty")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    GraphNode node "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" queue"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("poll")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    List"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" list "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("adjacency")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GraphNode n "),a("span",{attrs:{class:"token operator"}},[t._v(":")]),t._v(" list"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token operator"}},[t._v("!")]),t._v("set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("contains")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        queue"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("offer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("add")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    System"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("println")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("node"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("h2",{attrs:{id:"dfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dfs","aria-hidden":"true"}},[t._v("#")]),t._v(" DFS")]),t._v(" "),a("p",[t._v("和 BFS 类似的是，对于可以选择起始点的图，其 DFS 遍历也可能有 N 种。这里选择起始点为 A，那么其中的一种遍历可能为："),a("code",[t._v("A B D F E C")]),t._v("。")]),t._v(" "),a("p",[t._v("DFS 的遍历有一个回溯的过程，比如从 A 点一路走到底到 F 之后，发现 F 之后无路可走了，于是回溯到 D 点继续寻找到 E 点，诸如此类。")]),t._v(" "),a("h3",{attrs:{id:"stack-hashset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stack-hashset","aria-hidden":"true"}},[t._v("#")]),t._v(" Stack + HashSet")]),t._v(" "),a("p",[t._v("使用 Stack 来实现 DFS，可以保证下一个走的点一定是当前节点的邻接点。")]),t._v(" "),a("p",[t._v("假如以 A 点作为起始点，那么这个图的 DFS 遍历过程将是：")]),t._v(" "),a("ol",[a("li",[t._v("A 入栈，并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("A")]),t._v(" 出栈，遍历 A 的邻接点(B, C)，寻找不在 HashSet 中的点(说明该点还没有被遍历到)，B, C 入栈并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("C")]),t._v(" 出栈，遍历 C 的邻接点(A, B, E, D)，寻找不在 HashSet 中的点，E, D 入栈并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("D")]),t._v(" 出栈，遍历 D 的邻接点(B, C, E, F)，寻找不在 HashSet 中的点，F 入栈并加入 HashSet。")]),t._v(" "),a("li",[a("code",[t._v("F")]),t._v(" 出栈，遍历 F 的邻接点(D, E)，寻找不在 HashSet 中的点，没有符合要求的点。")]),t._v(" "),a("li",[a("code",[t._v("E")]),t._v(" 出栈，遍历 E 的邻接点(C, D, F)，寻找不在 HashSet 中的点，没有符合要求的点。")]),t._v(" "),a("li",[a("code",[t._v("B")]),t._v(" 出栈，遍历 B 的邻接点(A, C, D)，寻找不在 HashSet 中的点，没有符合要求的点。")]),t._v(" "),a("li",[t._v("栈为空，完成遍历。")])]),t._v(" "),a("p",[t._v("所以顺序为："),a("code",[t._v("A -> C -> D -> F -> E -> B")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"dfs-代码"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dfs-代码","aria-hidden":"true"}},[t._v("#")]),t._v(" DFS 代码")]),t._v(" "),a("div",{staticClass:"language-java extra-class"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("BFS")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GraphNode graph"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GraphNode root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  Stack"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" stack "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("Stack")]),a("span",{attrs:{class:"token operator"}},[t._v("<")]),a("span",{attrs:{class:"token operator"}},[t._v(">")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  Set"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" set "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("HashSet")]),a("span",{attrs:{class:"token operator"}},[t._v("<")]),a("span",{attrs:{class:"token operator"}},[t._v(">")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  stack"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("push")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("add")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token operator"}},[t._v("!")]),t._v("stack"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("isEmpty")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    GraphNode node "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" stack"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("pop")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    List"),a("span",{attrs:{class:"token generics function"}},[a("span",{attrs:{class:"token punctuation"}},[t._v("<")]),t._v("GraphNode"),a("span",{attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" list "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("adjacency")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("GraphNode n "),a("span",{attrs:{class:"token operator"}},[t._v(":")]),t._v(" list"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token operator"}},[t._v("!")]),t._v("set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("contains")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        stack"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("push")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        set"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("add")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    System"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("println")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("node"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("h2",{attrs:{id:"dijkstra"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dijkstra","aria-hidden":"true"}},[t._v("#")]),t._v(" Dijkstra")])])}],!1,null,null,null);o.options.__file="bfs+dfs.md";s.default=o.exports}}]);